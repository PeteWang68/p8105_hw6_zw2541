---
title: "p8105_hw6_zw2541"
author: "Zixu_Wang"
date: "11/22/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(modelr)
```

## Problem 1

### Read and clean the data

```{r}
homicides = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    solved = as.factor(ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1))) %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>% 
  mutate(
    victim_race = ifelse(victim_race == "White", "White", "non-White"),
    victim_race = factor(victim_race, levels = c("White", "non-White")),
    victim_age = as.numeric(victim_age))
```

### Fit a logistic regression model

```{r}
lm_baltimore =
  homicides %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(solved ~ victim_sex + victim_race + victim_age, family = binomial, data = .)

lm_baltimore %>% 
  broom::tidy(conf.int = TRUE) %>% 
  mutate(odds_ratio = exp(estimate),
         conf_low = exp(conf.low),
         conf_high = exp(conf.high)) %>% 
  select(term, odds_ratio, conf_low, conf_high) %>% 
  filter(term == "victim_racenon-White") %>% 
  knitr::kable(digits = 3)
```

The estimate of the adjusted odds ratio for solving homicides comparing non-white victims to white victims is 0.441 (keeping all other variables fixed), and the 95% confidence interval is (0.312, 0.620).

### Run glm for each of the cities 

```{r, warning = FALSE}
estimate_ci =   
  homicides %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(lm_everycity = map(data, ~glm(solved ~ victim_sex + victim_race + victim_age, family = binomial, data = .x)),
         lm_everycity_ci = map(lm_everycity, broom::confint_tidy),
         lm_everycity = map(lm_everycity, broom::tidy)) %>%
  select(-data) %>% 
  unnest() %>% 
  mutate(odds_ratio = exp(estimate),
         conf_low = exp(conf.low),
         conf_high = exp(conf.high)) %>% 
  filter(term == "victim_racenon-White") %>%
  select(city_state, odds_ratio, conf_low, conf_high)

estimate_ci 
```

### Create the plot

```{r}
estimate_ci %>% 
  mutate(city_state = fct_reorder(city_state, odds_ratio)) %>% 
  ggplot(aes(x = city_state, y = odds_ratio)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  geom_hline(yintercept = 1.0, linetype = "dashed") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "the Estimate Odds Ratio and Confidence Interval of Each City",
       x = "Odds Ratio",
       y = "City, State")
```

The plot above shows that only three cities (Durham, NC; Birmingham, AL; Tampa, FL) have the estimated odds ratio higher than one, which means that the odds of solving homicides of non-white victims is higher than the odds of solving homicides of white victims. Besides, 21 cities' higher confident interval limits are higher than one.

## Problem 2

### Read and clean the data

```{r}
birthweight = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    babysex = as.factor(babysex), 
    frace = as.factor(frace), 
    malform = as.factor(malform), 
    mrace = as.factor(mrace))

# check for missing data
sum(is.na(birthweight))
```
 
### Propose a regression model for birthweight

Use stepwise regression method (backward) to select the model.

```{r}
multiple_model = lm(bwt ~ ., data = birthweight)
step(multiple_model, direction = 'backward')
```

From the result above, we got the multiple regression model: bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight. Then we use this formula to create a multiple regression model.

```{r}
mlr_birthweight = lm(bwt ~ babysex + bhead + blength + delwt + fincome +  gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)

summary(mlr_birthweight)
```

### Plot of model residuals against fitted values 

```{r}
birthweight %>% 
  add_predictions(mlr_birthweight) %>% 
  add_residuals(mlr_birthweight) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = 0.25) + 
  geom_smooth() + 
  labs(title = "Model Residuals against Fitted Values",
        x = "Predictions",
        y = "Residuals")
```

The plot above is a Residuals vs Fitted/Predicted Values plot. It indicates that when the predicted values are bewteen 2000 and 4000, the residuals form a horizontal (linear) 'band' around zero and evenly distributed around 0. Therefore, this model is not fit when the predicted value is lower than 2000 or higher than 4000.

### Compare the model above to two others

Note: Model_1 is the model we created above, model_2 is the model using length at birth and gestational age as predictors (main effects only), and model_2 is the model using head circumference, length, sex, and all interactions (including the three-way interaction) between these.

```{r}
cross_validation = 
  crossv_mc(birthweight, 100) %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(model_1 = map(train, ~lm(bwt ~  babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model_3 = map(train, ~lm(bwt ~ babysex + blength + bhead + babysex * blength + babysex * bhead + blength * bhead + babysex * blength * bhead, data = .x)),
         rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
         rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)))

cross_validation %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +
  labs(title = "Comparison between Three Models",
        x = "Model",
        y = "RMSE")
```

From the plot above, we found that model_1 (which is the model we created) has the lowest RMSE and model_2 has the highest RMSE. Therefore, we conclude that our model is the best and model_2 is the worst.